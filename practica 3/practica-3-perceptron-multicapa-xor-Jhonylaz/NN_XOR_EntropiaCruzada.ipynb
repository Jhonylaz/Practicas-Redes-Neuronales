{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# XOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import json, matplotlib\n",
    "s = json.load( open(\"styles/bmh_matplotlibrc.json\") )\n",
    "matplotlib.rcParams.update(s)\n",
    "from IPython.core.pylabtools import figsize\n",
    "figsize(11, 5)\n",
    "colores = [\"#348ABD\", \"#A60628\",\"#06A628\"]\n",
    "from ipywidgets import interact, interact_manual, interactive, fixed\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Función de activación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistica(z):\n",
    "    \"\"\"\n",
    "    Devuelve la función logística evaluada componente por componente\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b03c0dacf074ab49d200870472f4dab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Output(),), _dom_classes=('widget-interact',))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib.pyplot import figure\n",
    "\n",
    "@interact()\n",
    "def plot_log():\n",
    "    z = np.arange(-5,5,0.1)\n",
    "    figure(figsize=(4,2))\n",
    "    plt.plot(z, logistica(z))\n",
    "    plt.xlabel(\"$z$\")\n",
    "    plt.ylabel(\"$\\sigma$\")\n",
    "    plt.title(\"$\\sigma = \\\\frac{1}{1 + e^{-z}}$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivada_logistica(z):\n",
    "    \"\"\"\n",
    "    Función que, dado un arreglo de valores z\n",
    "    calcula el valor de la derivada para cada entrada.\n",
    "    \"\"\"\n",
    "    g = logistica(z)\n",
    "    return g * (1 - g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd789023f3e941b59491c70315d5c1c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Output(),), _dom_classes=('widget-interact',))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact\n",
    "def plot_logp():\n",
    "    z = np.arange(-5,5,0.1)\n",
    "    figure(figsize=(4,2))\n",
    "    plt.plot(z, derivada_logistica(z))\n",
    "    plt.xlabel(\"$z$\")\n",
    "    plt.ylabel(\"$\\sigma'$\")\n",
    "    plt.title(\"$\\sigma' = \\sigma (1-\\sigma)$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivada_logistica_atajo(val_sigma):\n",
    "    \"\"\"\n",
    "    Función que, dado un arreglo de valores de sigma(z)\n",
    "    calcula el valor de la derivada para cada entrada.\n",
    "    \n",
    "    Si ya se cuenta con esos valores, es más eficiente\n",
    "    calcular esto directamente.\n",
    "    \"\"\"\n",
    "    return val_sigma*(1-val_sigma)\n",
    "    ## TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9eb03864d6424acbb4e48986554b010c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Output(),), _dom_classes=('widget-interact',))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact\n",
    "def plot_logpa():\n",
    "    z = np.arange(-5,5,0.1)\n",
    "    val = logistica(z)\n",
    "    figure(figsize=(4,2))\n",
    "    plt.plot(z, derivada_logistica_atajo(val))\n",
    "    plt.xlabel(\"$z$\")\n",
    "    plt.ylabel(\"$\\sigma'$\")\n",
    "    plt.title(\"$\\sigma' = \\sigma (1-\\sigma)$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Función de error: Entropía cruzada\n",
    "\n",
    "La función de error más sencilla utilizada para problemas de clasificación es la **entropía cruzada**\n",
    "\\begin{align}\n",
    "  J(\\Theta) =& - \\frac{1}{m} \\left[ \\sum_{i=1}^m \\sum_{k=1}^K   y_k^{(i)} \\log(h_\\Theta(x^{(i)}))_k  + (1 - y_k^{(i)}) \\log(1 - h_\\Theta(x^{(i)}))_k   \\right]    +  \\\\\n",
    "  & \\frac{\\lambda}{2m} \\sum_{l=1}^{L-1} \\sum_{i=1}^{s_L} \\sum_{j=1}^{s_{l+1}} (\\theta_{ji}^{(l)})^2\n",
    "\\end{align}\n",
    "donde:\n",
    "  * $K$ es el número de neuronas de salida.\n",
    "  * $s_l$ es el número de neuronas en la capa $l$.\n",
    "  * $m$ es el número de ejemplares de entrenamiento.\n",
    "  \n",
    "En su forma vectorizada, las componentes del gradiente están dadas por:\n",
    "\\begin{align}\n",
    "  \\delta^{(L)} &= (A^{(L)})^T - Y  \\\\\n",
    "  \\delta^{(L-1)} &= \\delta^{(L)} \\Theta_{[:,1:]}^{(L-1)} \\circ g'(z^{(L-1)}) \\\\\n",
    "  ... \\\\\n",
    "  \\delta^{(1)} &= \\delta^{(0)} \\Theta_{[:,1:]}^{(1)} \\circ g'(z^{(1)}) \\\\\n",
    "\\end{align}\n",
    "con:\n",
    "\\begin{align}\n",
    "  g'(z^{(l)}) = A^{(l)} \\circ (1 - A^{(l)})\n",
    "\\end{align}\n",
    "en general:\n",
    "\\begin{align}\n",
    "  \\Delta^{(l)} =& (\\delta^{(l+1)})^T A^{(l)}   &   \\nabla^{(l)} =& \\frac{1}{m}\\Delta^{(l)}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(y, h):\n",
    "    return - y * np.log(h) - (1 - y) * np.log(1 - h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98a73bf414f5481e946c24d49397b751",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Output(),), _dom_classes=('widget-interact',))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact\n",
    "def plot_crossentropy():\n",
    "    \"\"\"\n",
    "    Muestra como, en el rango donde está definida, la entropía cruzada tiene\n",
    "    sus valores más pequeños donde 'y' y 'h' coinciden, y su valor más alto\n",
    "    donde éstos son opuestos.\n",
    "    \"\"\"\n",
    "    fig = plt.figure()\n",
    "    ax = fig.gca(projection='3d')\n",
    "    \n",
    "    # Datos\n",
    "    X = np.arange(0.01, 1, 0.05)\n",
    "    Y = np.arange(0.01, 1, 0.05)\n",
    "    X, Y = np.meshgrid(X, Y)\n",
    "    CE = cross_entropy(X, Y)\n",
    "    \n",
    "    surf = ax.plot_surface(X, Y, CE, cmap=cm.coolwarm,\n",
    "                       linewidth=0, antialiased=False)\n",
    "    ax.set_xlabel(\"$y$\")\n",
    "    ax.set_ylabel(\"$h$\")\n",
    "    ax.set_title(\"Entropía cruzada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Red neuronal\n",
    "La red implementa encadenamiento hacia adelante (para evaluar) y hacia atrás (para entrenarse)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cuando se programa un algoritmo que depende de la generación de números aleatorios\n",
    "# es buena costumbre fijar la semilla a partir de la cual dichos números son generados.\n",
    "# De este modo el comportamiento sobre los datos de entrada será predecible mientras\n",
    "# se está desarrollando/probando el código.\n",
    "#\n",
    "# Una vez el código está listo, se debe usar variando los valores de esta semilla\n",
    "# para ver los efectos de la aleatoriedad simulada.\n",
    "\n",
    "np.random.seed(10)\n",
    "random.seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def productomatrix(x,y):\n",
    "    p=[]\n",
    "    for i in x:\n",
    "        x_1=[]\n",
    "        for j in y:\n",
    "            x_1.append(np.dot(i,j))\n",
    "        p.append(x_1)\n",
    "    return p "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multicapa:\n",
    "    \"\"\"\n",
    "    Red neuronal con tres capas:\n",
    "    \n",
    "    Entrada\n",
    "    Oculta\n",
    "    Salida\n",
    "    \n",
    "    Los parámetros (pesos) que conectan las capas se encuentran en matrices\n",
    "    con los nombres siguientes:\n",
    "    \n",
    "    Entrada -> self.Theta_0 -> Oculta\n",
    "    Oculta  -> self.Theta_1 -> Salida\n",
    "    \"\"\"\n",
    "    def __init__(self, n_entrada, n_ocultas, n_salidas):\n",
    "        \"\"\"\n",
    "        Inicializa la red neuronal, con pesos Theta_0 y Theta_1 aleatorios,\n",
    "        esta implementación debe incluir el uso de sesgos, por lo que éstos\n",
    "        no se cuentan en los parámetros siguientes, puedes incluirlos como\n",
    "        neuronas extra o en sus propias matrices, sólo sé consistente pues\n",
    "        esto afectará tu implementación.\n",
    "        \n",
    "        :param n_entrada: número de datos de entrada (sin contar el sesgo)\n",
    "        :param n_ocultas: número de neuronas ocultas\n",
    "        :param n_salidas: número de nueronas de salida\n",
    "        \"\"\"\n",
    "        random.seed(0)\n",
    "        self.entradas=n_entrada\n",
    "        self.ocultas=n_ocultas\n",
    "        self.salidas=n_salidas\n",
    "        W1=[]\n",
    "        for j in range(self.ocultas):\n",
    "            w1=[]\n",
    "            i=0\n",
    "            while(i<self.entradas+1):\n",
    "                w1.append(random.uniform(-0.5,0.5))\n",
    "                i+=1\n",
    "            W1.append(w1)  \n",
    "        W1=np.array(W1)    \n",
    "        self.Theta_0=W1\n",
    "        W2=[]\n",
    "        for j in range(self.salidas):\n",
    "            w2=[]\n",
    "            i=0\n",
    "            while(i<self.ocultas+1):\n",
    "                w2.append(random.uniform(-0.5,0.5))\n",
    "                i+=1\n",
    "                \n",
    "            W2.append(w2)\n",
    "        W2=np.array(W2)    \n",
    "        self.Theta_1=W2\n",
    "    def feed_forward(self, X, vector = None):\n",
    "        \"\"\" Calcula las salidas, dados los datos de entrada en forma de matriz.\n",
    "        Guarda los parámetros siguientes:\n",
    "        A0: activaciones de la capa de entrada, ya con sesgos\n",
    "        Z1: potenciales de la capa oculta, aún sin sesgo\n",
    "        A1: activaciones de la capa oculta, ya con sesgos\n",
    "        Z2: potenciales de la capa de salida\n",
    "        A2: activaciones de la capa de salida\n",
    "        \n",
    "        :param vector: [opcional] se utilizarán los pesos indicados en este\n",
    "                       vector en lugar de los pesos actuales de la red.\n",
    "        \"\"\"\n",
    "        ## TODO\n",
    "        X_1=X.tolist()\n",
    "        for i in range(len(X)):\n",
    "            X_1[i].insert(0,1)\n",
    "        A0=np.array(X_1)    \n",
    "        Z1=np.array(productomatrix(A0,self.Theta_0))\n",
    "        A1=logistica(Z1)\n",
    "        A1=A1.tolist()\n",
    "        for i in range(len(A1)):\n",
    "            A1[i].insert(0,1)\n",
    "        A1=np.array(A1)    \n",
    "        Z2=np.array(productomatrix(A1,self.Theta_1))\n",
    "        A2=logistica(Z2)\n",
    "        self.A0=A0\n",
    "        self.A1=A1\n",
    "        self.A2=A2\n",
    "    def back_propagate(self, X, Y, lambda_r = 0.0):\n",
    "        \"\"\" Calcula el error y su gradiente dados los pesos actuales de la red\n",
    "        y los resultados esperados.\n",
    "        \n",
    "        Guarda el error en el atributo self.error y el gradiente en matrices\n",
    "        self.Grad_1 y self.Grad_0, que tienen la misma forma de Theta_0 y Theta_1.\n",
    "        \n",
    "        :param X: matriz de entradas\n",
    "        :param Y: matriz de salidas deseadas\n",
    "        :param lambda_r: coeficiente de regularización\n",
    "        \"\"\"\n",
    "        ## TODO\n",
    "        J=0\n",
    "        J_theta= - np.sum(np.dot(Y.T, np.log(self.A2)) + np.dot((1 - Y).T, np.log(1 - self.A2))) / len(X)\n",
    "        the=np.array(self.vector_weights())\n",
    "        theta_sum=np.dot(the.T,the)[0][0]\n",
    "        Delta_2=(Y-self.A2)\n",
    "        Grad_1=-(1/len(X))*np.dot(self.A1.T,Delta_2)\n",
    "        gz_1=self.A1[:,1:]*(1-self.A1[:,1:])\n",
    "        Delta_1=np.dot(Delta_2,self.Theta_1[:,1:])*gz_1\n",
    "        Grad0=-(1/len(X))*np.dot(xor.A0[:,1:].T,Delta_1)\n",
    "        Grad0_b=- np.sum(Delta_1, axis=0, keepdims=True) / len(X)\n",
    "        Grad0=Grad0.T.tolist()\n",
    "        for i in range(len(self.Theta_0)):\n",
    "            Grad0[i].insert(0,Grad0_b[0,i])\n",
    "        Grad_0=np.array(Grad0)\n",
    "        k=J_theta+((lambda_r)/(2*(len(X))))*theta_sum\n",
    "        self.error=k\n",
    "        self.Grad_1=Grad_1.T\n",
    "        self.Grad_0=Grad_0 \n",
    "    def calc_error(self, X, Y, vector, lambda_r = 0.0):\n",
    "        \"\"\"\n",
    "        Calcula el error que se cometería utilizando los pesos en 'vector' en lugar\n",
    "        de los pesos actuales de la red.\n",
    "        \n",
    "        :returns: el error\n",
    "        \"\"\"\n",
    "        ## TODO\n",
    "    \n",
    "    def vector_weights(self):\n",
    "        \"\"\"\n",
    "        Acomoda a todos los parámetros en las matrices de sesgos y pesos, en un solo vector.\n",
    "        \n",
    "        :returns: vector de parámetros\n",
    "        \"\"\"\n",
    "        VP=[]\n",
    "        for i in self.Theta_0:\n",
    "            for j in i:\n",
    "                VP.append([j])\n",
    "        for i in self.Theta_1:\n",
    "            for j in i:\n",
    "                VP.append([j])\n",
    "        return VP\n",
    "        ## TODO\n",
    "    \n",
    "    def reconstruct_matrices(self, vector):\n",
    "        \"\"\"\n",
    "        Dado un vector, rearma matrices del tamaño de las matrices de sesgos y pesos.\n",
    "        \n",
    "        :returns: matrices de parámetros\n",
    "        \"\"\"\n",
    "        ## TODO\n",
    "        t1=[]\n",
    "        theta_1=[]\n",
    "        theta_0=[]\n",
    "        for i in range(self.ocultas):\n",
    "            t0=[]\n",
    "            for j in range(self.entradas+1):\n",
    "                t0.append(vector[j+i*(self.entradas+1)][0])\n",
    "            theta_0.append(t0)  \n",
    "        for i in range(self.ocultas+1):\n",
    "            t1.append(vector[i+2*(self.entradas+1)][0])\n",
    "        theta_1.append(t1)  \n",
    "        return theta_0,theta_1\n",
    "    def approx_gradient(self, X, Y, lambda_r = 0.0):\n",
    "        \"\"\"\n",
    "        Aproxima el valor del gradiente alrededor de los pesos actuales,\n",
    "        perturbando cada peso, uno por uno hasta estimar la variación alrededor\n",
    "        de cada peso.\n",
    "        \n",
    "        En este método se itera sobre cada peso w:\n",
    "        * Sea w - epsilon -> val1, se calcula el error e1 cometido por la red si w es\n",
    "                                   reemplazado por val1.\n",
    "        * Sea w + epsilon -> val2, se calcula el error e2 cometido por la red si w es\n",
    "                                   reemplazado por val2.\n",
    "        * La parcial correspondiente se estima como (val1 - val2)/(2 * epsilon)\n",
    "        \n",
    "        Este método sólo se utiliza para verificar que backpropagation esté bien\n",
    "        implementado, ya que en la práctica es muy lento y menos preciso.\n",
    "        \n",
    "        :returns: matrices que tienen la misma forma de Theta_0 y Theta_1, donde\n",
    "                  cada entrada es la estimación de la parcial del error con\n",
    "                  respecto al peso correspondiente\n",
    "        \"\"\"\n",
    "        ## TODO\n",
    "        g_params = [None, None]\n",
    "        epsilon = 0.0001\n",
    "        W=[self.Theta_0,self.Theta_1]\n",
    "        for t in range(len(W)):\n",
    "            param=W[t]\n",
    "            g_param = np.zeros(param.shape)\n",
    "            for i in range(param.shape[0]):\n",
    "                for j in range(param.shape[1]):\n",
    "                    temp = param[i,j]\n",
    "\n",
    "                    param[i,j] = temp - epsilon\n",
    "                    self.feed_forward(X)\n",
    "                    val1 = - np.sum(np.dot(Y.T, np.log(self.A2)) + np.dot((1 - Y).T, np.log(1 - self.A2))) / len(X)\n",
    "\n",
    "                    param[i,j] = temp + epsilon\n",
    "                    self.feed_forward(X)\n",
    "                    val2 = - np.sum(np.dot(Y.T, np.log(self.A2)) + np.dot((1 - Y).T, np.log(1 - self.A2))) / len(X)\n",
    "\n",
    "                    g_param[i,j] = (val2 - val1) / (2 * epsilon)\n",
    "                    param[i,j] = temp\n",
    "                g_params[t] = g_param\n",
    "        self.approx_0=g_params[0]\n",
    "        self.approx_1=g_params[1]\n",
    "        #return approx_0,approx_1      \n",
    "        \n",
    "    def gradient_descent(self, X, Y, alpha, ciclos=10, check_gradient = False, lambda_r = 0.0):\n",
    "        \"\"\" Evalúa y ajusta los pesos de la red, de acuerdo a los datos en X y los resultados\n",
    "        deseados en Y.  Al final grafica el error vs ciclo.  Si el entrenamiento es correcto\n",
    "        el error debe descender por cada iteración (ciclo).\n",
    "        \n",
    "        :param X: datos de entrada\n",
    "        :param Y: salidas deseadas\n",
    "        :param alpha: taza de aprendizaje\n",
    "        :param ciclos: número de veces que se realizarán ajustes para todo el conjunto de datos X\n",
    "        :param check_gradient: se calculará el gradiente con backpropagation y con aproximación por\n",
    "                               perturbaciones, imprimiendo los valores lado a lado para que puedan\n",
    "                               ser comparados.\n",
    "        :param lambda_r: coeficiente de regularización\n",
    "        \"\"\"\n",
    "        Z=[]\n",
    "        for i in range(ciclos):\n",
    "            self.feed_forward(X)\n",
    "            self.back_propagate(X,Y,lambda_r)\n",
    "            self.Theta_0=self.Theta_0-(alpha*(self.Grad_0))\n",
    "            self.Theta_1=self.Theta_1-(alpha*(self.Grad_1))\n",
    "            Z.append(self.error)\n",
    "        self.errores=Z    \n",
    "        if(check_gradient):\n",
    "            self.Theta_0=self.Theta_0+(alpha*(self.Grad_0))\n",
    "            self.Theta_1=self.Theta_1+(alpha*(self.Grad_1))\n",
    "            self.approx_gradient(X,Y)\n",
    "            print(\"Grad_0= \",self.Grad_0)\n",
    "            print(\"Approx_0= \",self.approx_0)\n",
    "            dif_1=self.Grad_0-self.approx_0\n",
    "            print(\"Diff= \",dif_1)\n",
    "            print(\"Grad_1= \",self.Grad_1)\n",
    "            print(\"Approx_1= \",self.approx_1)\n",
    "            dif_2=self.Grad_1-self.approx_1\n",
    "            print(\"Diff= \",dif_2)\n",
    "        x = np.arange(ciclos)\n",
    "        if ciclos > 1:\n",
    "            plt.title(\"Error\")\n",
    "            plt.xlabel(\"ciclo\")\n",
    "            plt.scatter(x, self.errores)    \n",
    "        return self.error\n",
    "    def print_output(self):\n",
    "        \"\"\"\n",
    "        Muestra en pantalla los valores de salida obtenidos en la última ejecución de feed_forward.\n",
    "        \"\"\"\n",
    "        O=X.tolist()\n",
    "        for i in range(len(self.A2)):\n",
    "            O[i].append(self.A2[i][0])\n",
    "        O=np.array(O)\n",
    "        print(O)\n",
    "        ## TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conjunto de datos\n",
    "\n",
    "Probaremos esta red con la función más sencilla que requiere más de un perceptrón: $XOR$.\n",
    "\n",
    "Para ello requerimos una red con la arquitectura siguiente:\n",
    "\n",
    "<img src=\"figuras/xor_sin_pesos.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.53951412]\n",
      " [0.         1.         0.54059727]\n",
      " [1.         0.         0.53651897]\n",
      " [1.         1.         0.5375526 ]]\n"
     ]
    }
   ],
   "source": [
    "# Datos\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])  # Entradas\n",
    "Y = np.array([[0], [1], [1], [0]])              # Salidas deseadas\n",
    "\n",
    "# Instanciación de la red\n",
    "xor = Multicapa(2, 2, 1)\n",
    "\n",
    "# Probamos qué valores calcula con una inicialización aleatoria\n",
    "xor.feed_forward(X)\n",
    "xor.print_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para verificar si la red funciona para lo que fue diseñada, no basta con reducir el error,\n",
    "debemos evaluar si calcula la función que queremos que calcule.  En este caso, queremos\n",
    "usarla para evaluar una función binaria.  Si realizamos un redondeo, veamos cuántas respuestas\n",
    "calcula correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "xor.back_propagate(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_correct(red, X, Y):\n",
    "    \"\"\" Conciderando que las etiquetas en Y son binarias (0 y 1)\n",
    "    la salida h de la red se tomará como 0 si h < 0.5\n",
    "    y como 1 si h >= 0.5\n",
    "    \"\"\"\n",
    "    red.feed_forward(X)\n",
    "    H = red.A2                        # Ojo: dependiendo de tu implementación, podrías no querer la .T\n",
    "    H = np.rint(H)\n",
    "    print(\"H = \", H)\n",
    "    print(\"Y = \", Y)\n",
    "    c = np.count_nonzero((H - Y)==0)\n",
    "    print(\"Se calcularon correctamente \", c, \"entradas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H =  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Y =  [[0]\n",
      " [1]\n",
      " [1]\n",
      " [0]]\n",
      "Se calcularon correctamente  2 entradas.\n"
     ]
    }
   ],
   "source": [
    "c = count_correct(xor, X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para poder implementar el algoritmo de optimización, las variables a optimizar, es decir, los pesos, deben ser colocados sobre un vector (o matriz columna, en este caso).  Es necesario convertir las matrices de pesos a vector y, para usar la red, revertir el proceso, reconstruyendo las matrices a partir del vector con los pesos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valores actuales de los pesos\n",
      "Theta_0:\n",
      " [[ 0.34442185  0.2579544  -0.07942842]\n",
      " [-0.24108325  0.01127472 -0.09506586]] \n",
      "\n",
      "Theta_1:\n",
      " [[ 0.28379859 -0.19668727 -0.02340305]]\n",
      "\n",
      "Matrices a vector de pesos: \n",
      "[[0.3444218515250481], [0.2579544029403025], [-0.079428419169155], [-0.24108324970703665], [0.01127472136860852], [-0.09506586254958571], [0.2837985890347726], [-0.19668727392107255], [-0.02340304584764419]]\n",
      "\n",
      "Reconstrucción de las matrices a partir del vector de pesos: \n",
      "[[0.3444218515250481, 0.2579544029403025, -0.079428419169155], [-0.24108324970703665, 0.01127472136860852, -0.09506586254958571]] [[0.2837985890347726, -0.19668727392107255, -0.02340304584764419]]\n"
     ]
    }
   ],
   "source": [
    "# Con esta casilla verifica que tus conversiones sean correctas\n",
    "\n",
    "print(\"Valores actuales de los pesos\")\n",
    "\n",
    "print(\"Theta_0:\\n\", xor.Theta_0, \"\\n\\nTheta_1:\\n\", xor.Theta_1)\n",
    "\n",
    "print(\"\\nMatrices a vector de pesos: \")\n",
    "print(xor.vector_weights())\n",
    "\n",
    "print(\"\\nReconstrucción de las matrices a partir del vector de pesos: \")\n",
    "T0, T1 = xor.reconstruct_matrices(xor.vector_weights())\n",
    "print(T0, T1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grad_0=  [[-1.85565376e-03 -9.65719189e-04 -6.26720898e-04]\n",
      " [-2.21281709e-04 -9.68293430e-05 -1.12552638e-04]]\n",
      "Approx_0=  [[-1.85565376e-03 -9.65719189e-04 -6.26720898e-04]\n",
      " [-2.21281708e-04 -9.68293423e-05 -1.12552638e-04]]\n",
      "Diff=  [[-8.24373556e-13 -6.26520537e-13  3.98233529e-13]\n",
      " [-1.24696694e-12 -6.62764006e-13 -1.18218694e-13]]\n",
      "Grad_1=  [[0.03854574 0.02344828 0.01655268]]\n",
      "Approx_1=  [[0.03854574 0.02344828 0.01655268]]\n",
      "Diff=  [[3.22165905e-11 7.70768172e-12 2.47806636e-12]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6961073967473654"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Con esta casilla revisa que backpropagation y aproximación por perturbaciones\n",
    "# den resultados semejantes.  Observa que se ejecuta sobre un solo ciclo pues es lento.\n",
    "\n",
    "xor.gradient_descent(X, Y, 0.3, 1, check_gradient = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c6fe502b3294180a85c4f6ddd695e13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1025, description='ciclos', max=2000, min=50), Button(description='Run I…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact_manual(ciclos = (50, 2000))\n",
    "def train_XOR(ciclos):\n",
    "    xor.gradient_descent(X, Y, 0.3, ciclos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.53951412]\n",
      " [0.         1.         0.54059727]\n",
      " [1.         0.         0.53651897]\n",
      " [1.         1.         0.5375526 ]]\n",
      "\n",
      "Theta_0 =  [[ 0.34442185  0.2579544  -0.07942842]\n",
      " [-0.24108325  0.01127472 -0.09506586]] \n",
      "\n",
      "Theta_1 [[ 0.28379859 -0.19668727 -0.02340305]]\n"
     ]
    }
   ],
   "source": [
    "# Después de haber sido correctamente entrenada, la red debe producir salidas\n",
    "# cercanas a las deseadas.  Dado que la función de activación es la sigmoide\n",
    "# nunca llegará a 0 o 1 exactamente.\n",
    "\n",
    "xor.feed_forward(X)\n",
    "xor.print_output()\n",
    "\n",
    "print(\"\\nTheta_0 = \", xor.Theta_0, \"\\n\\nTheta_1\", xor.Theta_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H =  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Y =  [[0]\n",
      " [1]\n",
      " [1]\n",
      " [0]]\n",
      "Se calcularon correctamente  2 entradas.\n"
     ]
    }
   ],
   "source": [
    "c = count_correct(xor, X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "xor_reg = Multicapa(2, 2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H =  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Y =  [[0]\n",
      " [1]\n",
      " [1]\n",
      " [0]]\n",
      "Se calcularon correctamente  2 entradas.\n"
     ]
    }
   ],
   "source": [
    "c = count_correct(xor_reg, X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grad_0=  [[-1.85565376e-03 -9.65719189e-04 -6.26720898e-04]\n",
      " [-2.21281709e-04 -9.68293430e-05 -1.12552638e-04]]\n",
      "Approx_0=  [[-1.85565376e-03 -9.65719189e-04 -6.26720898e-04]\n",
      " [-2.21281708e-04 -9.68293423e-05 -1.12552638e-04]]\n",
      "Diff=  [[-8.24373556e-13 -6.26520537e-13  3.98233529e-13]\n",
      " [-1.24696694e-12 -6.62764006e-13 -1.18218694e-13]]\n",
      "Grad_1=  [[0.03854574 0.02344828 0.01655268]]\n",
      "Approx_1=  [[0.03854574 0.02344828 0.01655268]]\n",
      "Diff=  [[3.22165905e-11 7.70768172e-12 2.47806636e-12]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7032049609194603"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xor.gradient_descent(X, Y, 0.3, 1, check_gradient = True, lambda_r = 0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "565b98d704444c2ea00f3ef855c9bccf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1025, description='ciclos', max=2000, min=50), Button(description='Run I…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact_manual(ciclos = (50, 2000))\n",
    "def train_XOR_reg(ciclos):\n",
    "    # Prueba diferentes valores de lambda_r ¿qué tanto lo puedes incrementar si matar a la red?\n",
    "    xor_reg.gradient_descent(X, Y, 0.3, ciclos, lambda_r = 0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.04876228]\n",
      " [0.         1.         0.94393514]\n",
      " [1.         0.         0.95123573]\n",
      " [1.         1.         0.0407789 ]]\n",
      "Theta_0 =  [[ 2.97929354  5.75657305 -5.64874085]\n",
      " [-2.44817156  4.59372451 -4.90189787]] \n",
      "Theta_1 [[ 3.28646135 -7.22008093  7.71092354]]\n"
     ]
    }
   ],
   "source": [
    "xor_reg.feed_forward(X)\n",
    "xor_reg.print_output()\n",
    "print(\"Theta_0 = \", xor_reg.Theta_0, \"\\nTheta_1\", xor_reg.Theta_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H =  [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "Y =  [[0]\n",
      " [1]\n",
      " [1]\n",
      " [0]]\n",
      "Se calcularon correctamente  4 entradas.\n"
     ]
    }
   ],
   "source": [
    "c = count_correct(xor_reg, X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    img {\n",
       "        display: block;\n",
       "\t\tmargin-left: auto;\n",
       "\t\tmargin-right: auto;\n",
       "\t}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "def css_styling():\n",
    "    styles = open(\"styles/custom.css\", \"r\").read() #or edit path to custom.css\n",
    "    return HTML(styles)\n",
    "css_styling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
